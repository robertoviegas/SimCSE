{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "386bd2c4",
   "metadata": {},
   "source": [
    "# SimCSE Supervisionado - Código Completo no Notebook\n",
    "Este notebook contém o código completo do `train.py` embutido diretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69869b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/workspaces/SimCSE\")\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Union, List, Dict, Tuple\n",
    "import torch\n",
    "import collections\n",
    "import random\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_MASKED_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    EvalPrediction,\n",
    "    BertModel,\n",
    "    BertForPreTraining,\n",
    "    RobertaModel\n",
    ")\n",
    "from transformers.tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTrainedTokenizerBase\n",
    "from transformers.trainer_utils import is_main_process\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
    "from transformers.file_utils import cached_property, torch_required, is_torch_available, is_torch_tpu_available\n",
    "from simcse.models import RobertaForCL, BertForCL\n",
    "from simcse.trainers import CLTrainer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Huggingface's original arguments\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The model checkpoint for weights initialization.\"\n",
    "            \"Don't set if you want to train a model from scratch.\"\n",
    "        },\n",
    "    )\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # SimCSE's arguments\n",
    "    temp: float = field(\n",
    "        default=0.05,\n",
    "        metadata={\n",
    "            \"help\": \"Temperature for softmax.\"\n",
    "        }\n",
    "    )\n",
    "    pooler_type: str = field(\n",
    "        default=\"cls\",\n",
    "        metadata={\n",
    "            \"help\": \"What kind of pooler to use (cls, cls_before_pooler, avg, avg_top2, avg_first_last).\"\n",
    "        }\n",
    "    ) \n",
    "    hard_negative_weight: float = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            \"help\": \"The **logit** of weight for hard negatives (only effective if hard negatives are used).\"\n",
    "        }\n",
    "    )\n",
    "    do_mlm: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to use MLM auxiliary objective.\"\n",
    "        }\n",
    "    )\n",
    "    mlm_weight: float = field(\n",
    "        default=0.1,\n",
    "        metadata={\n",
    "            \"help\": \"Weight for MLM auxiliary objective (only effective if --do_mlm).\"\n",
    "        }\n",
    "    )\n",
    "    mlp_only_train: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Use MLP only during training\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    # Huggingface's original arguments. \n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    validation_split_percentage: Optional[int] = field(\n",
    "        default=5,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "\n",
    "    # SimCSE's arguments\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, \n",
    "        metadata={\"help\": \"The training data file (.txt or .csv).\"}\n",
    "    )\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=32,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated.\"\n",
    "        },\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
    "        },\n",
    "    )\n",
    "    mlm_probability: float = field(\n",
    "        default=0.15, \n",
    "        metadata={\"help\": \"Ratio of tokens to mask for MLM (only effective if --do_mlm)\"}\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OurTrainingArguments(TrainingArguments):\n",
    "    # Evaluation\n",
    "    ## By default, we evaluate STS (dev) during training (for selecting best checkpoints) and evaluate \n",
    "    ## both STS and transfer tasks (dev) at the end of training. Using --eval_transfer will allow evaluating\n",
    "    ## both STS and transfer tasks (dev) during training.\n",
    "    eval_transfer: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Evaluate transfer task dev sets (in validation).\"}\n",
    "    )\n",
    "\n",
    "    @cached_property\n",
    "    @torch_required\n",
    "    def _setup_devices(self) -> \"torch.device\":\n",
    "        logger.info(\"PyTorch: setting up devices\")\n",
    "        if self.no_cuda:\n",
    "            device = torch.device(\"cpu\")\n",
    "            self._n_gpu = 0\n",
    "        elif is_torch_tpu_available():\n",
    "            import torch_xla.core.xla_model as xm\n",
    "            device = xm.xla_device()\n",
    "            self._n_gpu = 0\n",
    "        elif self.local_rank == -1:\n",
    "            # if n_gpu is > 1 we'll use nn.DataParallel.\n",
    "            # If you only want to use a specific subset of GPUs use `CUDA_VISIBLE_DEVICES=0`\n",
    "            # Explicitly set CUDA to the first (index 0) CUDA device, otherwise `set_device` will\n",
    "            # trigger an error that a device index is missing. Index 0 takes into account the\n",
    "            # GPUs available in the environment, so `CUDA_VISIBLE_DEVICES=1,2` with `cuda:0`\n",
    "            # will use the first GPU in that env, i.e. GPU#1\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            # Sometimes the line in the postinit has not been run before we end up here, so just checking we're not at\n",
    "            # the default value.\n",
    "            self._n_gpu = torch.cuda.device_count()\n",
    "        else:\n",
    "            # Here, we'll use torch.distributed.\n",
    "            # Initializes the distributed backend which will take care of synchronizing nodes/GPUs\n",
    "            #\n",
    "            # deepspeed performs its own DDP internally, and requires the program to be started with:\n",
    "            # deepspeed  ./program.py\n",
    "            # rather than:\n",
    "            # python -m torch.distributed.launch --nproc_per_node=2 ./program.py\n",
    "            if self.deepspeed:\n",
    "                from .integrations import is_deepspeed_available\n",
    "\n",
    "                if not is_deepspeed_available():\n",
    "                    raise ImportError(\"--deepspeed requires deepspeed: `pip install deepspeed`.\")\n",
    "                import deepspeed\n",
    "\n",
    "                deepspeed.init_distributed()\n",
    "            else:\n",
    "                torch.distributed.init_process_group(backend=\"nccl\")\n",
    "            device = torch.device(\"cuda\", self.local_rank)\n",
    "            self._n_gpu = 1\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.set_device(device)\n",
    "\n",
    "        return device\n",
    "\n",
    "\n",
    "def main():\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, OurTrainingArguments))\n",
    "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "        # If we pass only one argument to the script and it's the path to a json file,\n",
    "        # let's parse it to get our arguments.\n",
    "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
    "    else:\n",
    "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    if (\n",
    "        os.path.exists(training_args.output_dir)\n",
    "        and os.listdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty.\"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if is_main_process(training_args.local_rank) else logging.WARN,\n",
    "    )\n",
    "\n",
    "    # Log on each process the small summary:\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
    "    if is_main_process(training_args.local_rank):\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "        transformers.utils.logging.enable_default_handler()\n",
    "        transformers.utils.logging.enable_explicit_format()\n",
    "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "    # Set seed before initializing model.\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
    "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "    # (the dataset will be downloaded automatically from the datasets Hub\n",
    "    #\n",
    "    # For CSV/JSON files, this script will use the column called 'text' or the first column. You can easily tweak this\n",
    "    # behavior (see below)\n",
    "    #\n",
    "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "    # download the dataset.\n",
    "    data_files = {}\n",
    "    if data_args.train_file is not None:\n",
    "        data_files[\"train\"] = data_args.train_file\n",
    "    extension = data_args.train_file.split(\".\")[-1]\n",
    "    if extension == \"txt\":\n",
    "        extension = \"text\"\n",
    "    if extension == \"csv\":\n",
    "        datasets = load_dataset(extension, data_files=data_files, cache_dir=\"./data/\", delimiter=\"\\t\" if \"tsv\" in data_args.train_file else \",\")\n",
    "    else:\n",
    "        datasets = load_dataset(extension, data_files=data_files, cache_dir=\"./data/\")\n",
    "\n",
    "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # Distributed training:\n",
    "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "    config_kwargs = {\n",
    "        \"cache_dir\": model_args.cache_dir,\n",
    "        \"revision\": model_args.model_revision,\n",
    "        \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "    }\n",
    "    if model_args.config_name:\n",
    "        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n",
    "    elif model_args.model_name_or_path:\n",
    "        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "    else:\n",
    "        config = CONFIG_MAPPING[model_args.model_type]()\n",
    "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "\n",
    "    tokenizer_kwargs = {\n",
    "        \"cache_dir\": model_args.cache_dir,\n",
    "        \"use_fast\": model_args.use_fast_tokenizer,\n",
    "        \"revision\": model_args.model_revision,\n",
    "        \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "    }\n",
    "    if model_args.tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n",
    "    elif model_args.model_name_or_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "        )\n",
    "\n",
    "    if model_args.model_name_or_path:\n",
    "        if 'roberta' in model_args.model_name_or_path:\n",
    "            model = RobertaForCL.from_pretrained(\n",
    "                model_args.model_name_or_path,\n",
    "                from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "                config=config,\n",
    "                cache_dir=model_args.cache_dir,\n",
    "                revision=model_args.model_revision,\n",
    "                use_auth_token=True if model_args.use_auth_token else None,\n",
    "                model_args=model_args                  \n",
    "            )\n",
    "        elif 'bert' in model_args.model_name_or_path:\n",
    "            model = BertForCL.from_pretrained(\n",
    "                model_args.model_name_or_path,\n",
    "                from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "                config=config,\n",
    "                cache_dir=model_args.cache_dir,\n",
    "                revision=model_args.model_revision,\n",
    "                use_auth_token=True if model_args.use_auth_token else None,\n",
    "                model_args=model_args\n",
    "            )\n",
    "            if model_args.do_mlm:\n",
    "                pretrained_model = BertForPreTraining.from_pretrained(model_args.model_name_or_path)\n",
    "                model.lm_head.load_state_dict(pretrained_model.cls.predictions.state_dict())\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        logger.info(\"Training new model from scratch\")\n",
    "        model = AutoModelForMaskedLM.from_config(config)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Prepare features\n",
    "    column_names = datasets[\"train\"].column_names\n",
    "    sent2_cname = None\n",
    "    if len(column_names) == 2:\n",
    "        # Pair datasets\n",
    "        sent0_cname = column_names[0]\n",
    "        sent1_cname = column_names[1]\n",
    "    elif len(column_names) == 3:\n",
    "        # Pair datasets with hard negatives\n",
    "        sent0_cname = column_names[0]\n",
    "        sent1_cname = column_names[1]\n",
    "        sent2_cname = column_names[2]\n",
    "    elif len(column_names) == 1:\n",
    "        # Unsupervised datasets\n",
    "        sent0_cname = column_names[0]\n",
    "        sent1_cname = column_names[0]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def prepare_features(examples):\n",
    "        # padding = longest (default)\n",
    "        #   If no sentence in the batch exceed the max length, then use\n",
    "        #   the max sentence length in the batch, otherwise use the \n",
    "        #   max sentence length in the argument and truncate those that\n",
    "        #   exceed the max length.\n",
    "        # padding = max_length (when pad_to_max_length, for pressure test)\n",
    "        #   All sentences are padded/truncated to data_args.max_seq_length.\n",
    "        total = len(examples[sent0_cname])\n",
    "\n",
    "        # Avoid \"None\" fields \n",
    "        for idx in range(total):\n",
    "            if examples[sent0_cname][idx] is None:\n",
    "                examples[sent0_cname][idx] = \" \"\n",
    "            if examples[sent1_cname][idx] is None:\n",
    "                examples[sent1_cname][idx] = \" \"\n",
    "        \n",
    "        sentences = examples[sent0_cname] + examples[sent1_cname]\n",
    "\n",
    "        # If hard negative exists\n",
    "        if sent2_cname is not None:\n",
    "            for idx in range(total):\n",
    "                if examples[sent2_cname][idx] is None:\n",
    "                    examples[sent2_cname][idx] = \" \"\n",
    "            sentences += examples[sent2_cname]\n",
    "\n",
    "        sent_features = tokenizer(\n",
    "            sentences,\n",
    "            max_length=data_args.max_seq_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\" if data_args.pad_to_max_length else False,\n",
    "        )\n",
    "\n",
    "        features = {}\n",
    "        if sent2_cname is not None:\n",
    "            for key in sent_features:\n",
    "                features[key] = [[sent_features[key][i], sent_features[key][i+total], sent_features[key][i+total*2]] for i in range(total)]\n",
    "        else:\n",
    "            for key in sent_features:\n",
    "                features[key] = [[sent_features[key][i], sent_features[key][i+total]] for i in range(total)]\n",
    "            \n",
    "        return features\n",
    "\n",
    "    if training_args.do_train:\n",
    "        train_dataset = datasets[\"train\"].map(\n",
    "            prepare_features,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "        )\n",
    "\n",
    "    # Data collator\n",
    "    @dataclass\n",
    "    class OurDataCollatorWithPadding:\n",
    "\n",
    "        tokenizer: PreTrainedTokenizerBase\n",
    "        padding: Union[bool, str, PaddingStrategy] = True\n",
    "        max_length: Optional[int] = None\n",
    "        pad_to_multiple_of: Optional[int] = None\n",
    "        mlm: bool = True\n",
    "        mlm_probability: float = data_args.mlm_probability\n",
    "\n",
    "        def __call__(self, features: List[Dict[str, Union[List[int], List[List[int]], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "            special_keys = ['input_ids', 'attention_mask', 'token_type_ids', 'mlm_input_ids', 'mlm_labels']\n",
    "            bs = len(features)\n",
    "            if bs > 0:\n",
    "                num_sent = len(features[0]['input_ids'])\n",
    "            else:\n",
    "                return\n",
    "            flat_features = []\n",
    "            for feature in features:\n",
    "                for i in range(num_sent):\n",
    "                    flat_features.append({k: feature[k][i] if k in special_keys else feature[k] for k in feature})\n",
    "\n",
    "            batch = self.tokenizer.pad(\n",
    "                flat_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            if model_args.do_mlm:\n",
    "                batch[\"mlm_input_ids\"], batch[\"mlm_labels\"] = self.mask_tokens(batch[\"input_ids\"])\n",
    "\n",
    "            batch = {k: batch[k].view(bs, num_sent, -1) if k in special_keys else batch[k].view(bs, num_sent, -1)[:, 0] for k in batch}\n",
    "\n",
    "            if \"label\" in batch:\n",
    "                batch[\"labels\"] = batch[\"label\"]\n",
    "                del batch[\"label\"]\n",
    "            if \"label_ids\" in batch:\n",
    "                batch[\"labels\"] = batch[\"label_ids\"]\n",
    "                del batch[\"label_ids\"]\n",
    "\n",
    "            return batch\n",
    "        \n",
    "        def mask_tokens(\n",
    "            self, inputs: torch.Tensor, special_tokens_mask: Optional[torch.Tensor] = None\n",
    "        ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "            \"\"\"\n",
    "            Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "            \"\"\"\n",
    "            inputs = inputs.clone()\n",
    "            labels = inputs.clone()\n",
    "            # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "            probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "            if special_tokens_mask is None:\n",
    "                special_tokens_mask = [\n",
    "                    self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "                ]\n",
    "                special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "            else:\n",
    "                special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "            probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "            masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "            labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "            # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "            indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "            inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "            # 10% of the time, we replace masked input tokens with random word\n",
    "            indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "            random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "            inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "            # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "            return inputs, labels\n",
    "\n",
    "    data_collator = default_data_collator if data_args.pad_to_max_length else OurDataCollatorWithPadding(tokenizer)\n",
    "\n",
    "    trainer = CLTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset if training_args.do_train else None,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    trainer.model_args = model_args\n",
    "\n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        model_path = (\n",
    "            model_args.model_name_or_path\n",
    "            if (model_args.model_name_or_path is not None and os.path.isdir(model_args.model_name_or_path))\n",
    "            else None\n",
    "        )\n",
    "        train_result = trainer.train(model_path=model_path)\n",
    "        trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "        output_train_file = os.path.join(training_args.output_dir, \"train_results.txt\")\n",
    "        if trainer.is_world_process_zero():\n",
    "            with open(output_train_file, \"w\") as writer:\n",
    "                logger.info(\"***** Train results *****\")\n",
    "                for key, value in sorted(train_result.metrics.items()):\n",
    "                    logger.info(f\"  {key} = {value}\")\n",
    "                    writer.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "            # Need to save the state, since Trainer.save_model saves only the tokenizer with the model\n",
    "            trainer.state.save_to_json(os.path.join(training_args.output_dir, \"trainer_state.json\"))\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if training_args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "        results = trainer.evaluate(eval_senteval_transfer=True)\n",
    "\n",
    "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
    "        if trainer.is_world_process_zero():\n",
    "            with open(output_eval_file, \"w\") as writer:\n",
    "                logger.info(\"***** Eval results *****\")\n",
    "                for key, value in sorted(results.items()):\n",
    "                    logger.info(f\"  {key} = {value}\")\n",
    "                    writer.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def _mp_fn(index):\n",
    "    # For xla_spawn (TPUs)\n",
    "    main()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16cb331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substituir argumentos por uma instância da classe para uso direto\n",
    "class Args:\n",
    "    model_name_or_path = \"bert-base-uncased\"\n",
    "    train_file = \"data/nli_for_simcse.csv\"\n",
    "    output_dir = \"result/my-sup-simcse-bert-base-uncased\"\n",
    "    num_train_epochs = 3\n",
    "    per_device_train_batch_size = 128\n",
    "    learning_rate = 5e-5\n",
    "    max_seq_length = 32\n",
    "    evaluation_strategy = \"steps\"\n",
    "    metric_for_best_model = \"stsb_spearman\"\n",
    "    load_best_model_at_end = True\n",
    "    eval_steps = 125\n",
    "    pooler_type = \"cls\"\n",
    "    overwrite_output_dir = True\n",
    "    temp = 0.05\n",
    "    do_train = True\n",
    "    do_eval = True\n",
    "    fp16 = True\n",
    "\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "149edd9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/--f=/root/.local/share/jupyter/runtime/kernel-v2-336ep1Wvt94TJRp.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 260\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m parser \u001b[38;5;241m=\u001b[39m HfArgumentParser((ModelArguments, DataTrainingArguments, OurTrainingArguments))\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sys\u001b[38;5;241m.\u001b[39margv) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# If we pass only one argument to the script and it's the path to a json file,\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# let's parse it to get our arguments.\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m     model_args, data_args, training_args \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     model_args, data_args, training_args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args_into_dataclasses()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/hf_argparser.py:175\u001b[0m, in \u001b[0;36mHfArgumentParser.parse_json_file\u001b[0;34m(self, json_file)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse_json_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, json_file: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[DataClass, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m    171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    Alternative helper method that does not use `argparse` at all, instead loading a json file and populating the\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m    dataclass types.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    176\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dtype \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataclass_types:\n",
      "File \u001b[0;32m/usr/lib/python3.8/pathlib.py:1236\u001b[0m, in \u001b[0;36mPath.read_text\u001b[0;34m(self, encoding, errors)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;124;03m    Open the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1236\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m   1237\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.8/pathlib.py:1222\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_closed()\n\u001b[0;32m-> 1222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m               \u001b[49m\u001b[43mopener\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/pathlib.py:1078\u001b[0m, in \u001b[0;36mPath._opener\u001b[0;34m(self, name, flags, mode)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_opener\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, flags, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0o666\u001b[39m):\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# A stub for the opener argument to built-in open()\u001b[39;00m\n\u001b[0;32m-> 1078\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/--f=/root/.local/share/jupyter/runtime/kernel-v2-336ep1Wvt94TJRp.json'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
